{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from Flick30KDataset import Flickr30kDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "# Dataset and DataLoader\n",
    "images_dir = 'data/flickr30k_images'\n",
    "captions_file = 'data/results.csv'\n",
    "dataset = Flickr30kDataset(images_dir, captions_file, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one image and its caption\n",
    "image, caption = dataset[3]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rearrange dimensions for proper display\n",
    "image_display = image.permute(1, 2, 0)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image_display)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Print the caption\n",
    "print(\"Caption for the first image:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTokenizer, CLIPModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Encoder import VAE_Encoder\n",
    "from Decoder import VAE_Decoder\n",
    "from Diffusion import Diffusion\n",
    "from Clip import CLIP\n",
    "\n",
    "def train():\n",
    "    images_dir = 'data/flickr30k_images'\n",
    "    captions_dir = 'data/results.csv'  # Assuming captions are in CSV format\n",
    "    batch_size = 4\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    device = 'cpu'\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    dataset = Flickr30kDataset(images_dir, captions_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    vae_encoder = VAE_Encoder().to(device)\n",
    "    vae_decoder = VAE_Decoder().to(device)\n",
    "    diffusion = Diffusion().to(device)\n",
    "    clip_model = CLIP().to(device)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': vae_encoder.parameters()},\n",
    "        {'params': vae_decoder.parameters()},\n",
    "        {'params': diffusion.parameters()},\n",
    "    ], lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        vae_encoder.train()\n",
    "        vae_decoder.train()\n",
    "        diffusion.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, captions in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            captions = [caption[0] for caption in captions]  # Ensure captions are strings\n",
    "            tokenized_captions = tokenizer(captions, padding=True, return_tensors=\"pt\").input_ids.to(device)\n",
    "            text_features = clip_model(tokenized_captions).mean(dim=1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            noise = torch.randn((images.size(0), 4, images.size(2) // 8, images.size(3) // 8)).to(device)\n",
    "            latents = vae_encoder(images, noise)\n",
    "            outputs = diffusion(latents, text_features, torch.randn_like(latents))\n",
    "            outputs = vae_decoder(outputs)\n",
    "            loss = criterion(outputs, images)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "\n",
    "    print('Training finished.')\n",
    "\n",
    "def generate_image(prompt, clip_model, tokenizer, vae_encoder, vae_decoder, diffusion, device):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    text_features = clip_model(tokenized_prompt).mean(dim=1)\n",
    "    latents = torch.randn((1, 512, 512)).to(device)  # Initialize random latent tensor\n",
    "    with torch.no_grad():\n",
    "        latents = diffusion(latents, text_features, torch.randn_like(latents))\n",
    "        generated_image = vae_decoder(latents)\n",
    "    return generated_image\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"man drinking water\"\n",
    "device = 'cpu'\n",
    "vae_encoder = VAE_Encoder().to(device)\n",
    "vae_decoder = VAE_Decoder().to(device)\n",
    "diffusion = Diffusion().to(device)\n",
    "clip_model = CLIP().to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "generated_image = generate_image(prompt, clip_model, tokenizer, vae_encoder, vae_decoder, diffusion, device)\n",
    "transforms.ToPILImage()(generated_image.squeeze()).save(f'{prompt}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
